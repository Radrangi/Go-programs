{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a372f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark Session\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea60f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reading CSV file\n",
    "df_csv = spark.read.csv(\"/path/to/csv/file\", header=True, inferSchema=True)\n",
    "\n",
    "# Reading Parquet file\n",
    "df_parquet = spark.read.parquet(\"/path/to/parquet/file\")\n",
    "\n",
    "# Reading from a database\n",
    "df_db = spark.read.format(\"jdbc\").option(\"url\", \"jdbc:postgresql://host:port/dbname\")\\\n",
    "                   .option(\"dbtable\", \"table_name\")\\\n",
    "                   .option(\"user\", \"username\")\\\n",
    "                   .option(\"password\", \"password\").load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b10670",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sample DataFrame\n",
    "data = [(\"Alice\", None), (\"Bob\", 45), (\"Cathy\", 29)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "\n",
    "# Dropping rows with missing values\n",
    "df_clean = df.dropna()\n",
    "\n",
    "# Filling missing values\n",
    "df_filled = df.fillna({'Age': 0})\n",
    "\n",
    "# Type casting\n",
    "df_casted = df.withColumn(\"Age\", df[\"Age\"].cast(\"Integer\"))\n",
    "\n",
    "# Basic transformations\n",
    "df_filtered = df.filter(df['Age'] > 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1d825c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Group by and aggregation\n",
    "df_aggregated = df.groupBy(\"Name\").agg({\"Age\": \"sum\"})\n",
    "\n",
    "# Joins\n",
    "df1 = spark.createDataFrame([(\"Alice\", 34), (\"Bob\", 45)], [\"Name\", \"Age\"])\n",
    "df2 = spark.createDataFrame([(\"Alice\", \"F\"), (\"Bob\", \"M\")], [\"Name\", \"Gender\"])\n",
    "df_joined = df1.join(df2, df1[\"Name\"] == df2[\"Name\"], \"inner\")\n",
    "\n",
    "# Pivoting\n",
    "df_pivot = df.groupBy(\"Name\").pivot(\"Age\").sum(\"Age\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ba5f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Adding a new column\n",
    "df_enriched = df.withColumn(\"Country\", lit(\"USA\"))\n",
    "\n",
    "# Using UDF for custom transformations\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def custom_function(value):\n",
    "    return value.upper() if value else None\n",
    "\n",
    "udf_custom_function = udf(custom_function, StringType())\n",
    "df_enriched = df.withColumn(\"Name_Upper\", udf_custom_function(df[\"Name\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84019244",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SQL queries\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "df_sql = spark.sql(\"SELECT Name, COUNT(*) FROM people GROUP BY Name\")\n",
    "\n",
    "# Aggregations using DataFrame API\n",
    "df_aggregated = df.groupBy(\"Name\").count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58bc88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Writing to CSV\n",
    "df.write.csv(\"/path/to/output/csv\")\n",
    "\n",
    "# Writing to Parquet\n",
    "df.write.parquet(\"/path/to/output/parquet\")\n",
    "\n",
    "# Writing to a database\n",
    "df.write.format(\"jdbc\").option(\"url\", \"jdbc:postgresql://host:port/dbname\")\\\n",
    "                       .option(\"dbtable\", \"table_name\")\\\n",
    "                       .option(\"user\", \"username\")\\\n",
    "                       .option(\"password\", \"password\").save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d13700",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, col\n",
    "\n",
    "# Define a window specification\n",
    "windowSpec = Window.partitionBy(\"Name\").orderBy(\"Age\")\n",
    "df_with_row_number = df.withColumn(\"row_number\", row_number().over(windowSpec))\n",
    "\n",
    "# Define a Python function\n",
    "def to_upper(s):\n",
    "    return s.upper() if s else None\n",
    "\n",
    "# Register the function as a UDF\n",
    "to_upper_udf = udf(to_upper, StringType())\n",
    "\n",
    "# Use the UDF in DataFrame operations\n",
    "df_with_upper = df.withColumn(\"Name_Upper\", to_upper_udf(col(\"Name\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d651fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Perform a broadcast join\n",
    "df_joined = df1.join(broadcast(df2), df1[\"Name\"] == df2[\"Name\"])\n",
    "\n",
    "# Cache a DataFrame\n",
    "df.cache()\n",
    "\n",
    "# Persist a DataFrame with a specific storage level\n",
    "from pyspark import StorageLevel\n",
    "df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "# Perform actions to materialize the cache\n",
    "df.count()\n",
    "\n",
    "# Repartition a DataFrame\n",
    "df_repartitioned = df.repartition(10)\n",
    "\n",
    "# Coalesce a DataFrame\n",
    "df_coalesced = df.coalesce(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89283f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import explode, split, struct, create_map, lit\n",
    "\n",
    "# Create a DataFrame with an array column\n",
    "df_with_array = df.select(split(col(\"Name\"), \",\").alias(\"Name_Array\"))\n",
    "\n",
    "# Explode the array into individual rows\n",
    "df_exploded = df_with_array.select(explode(col(\"Name_Array\")).alias(\"Name\"))\n",
    "\n",
    "# Create a DataFrame with a struct column\n",
    "df_with_struct = df.select(struct(col(\"Name\"), col(\"Age\")).alias(\"Person\"))\n",
    "\n",
    "# Select fields from the struct\n",
    "df_selected = df_with_struct.select(col(\"Person.Name\"), col(\"Person.Age\"))\n",
    "\n",
    "# Create a DataFrame with a map column\n",
    "df_with_map = df.select(create_map(lit(\"Name\"), col(\"Name\"), lit(\"Age\"), col(\"Age\")).alias(\"Name_Age_Map\"))\n",
    "\n",
    "# Access elements from the map\n",
    "df_selected = df_with_map.select(col(\"Name_Age_Map\")[\"Name\"].alias(\"Name\"), col(\"Name_Age_Map\")[\"Age\"].alias(\"Age\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba628f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Sample DataFrame\n",
    "data = [(\"Alice\", 1), (\"Bob\", 0), (\"Cathy\", 1)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Label\"])\n",
    "\n",
    "# StringIndexer for categorical features\n",
    "indexer = StringIndexer(inputCol=\"Name\", outputCol=\"NameIndex\")\n",
    "df_indexed = indexer.fit(df).transform(df)\n",
    "\n",
    "# OneHotEncoder for indexed categorical features\n",
    "encoder = OneHotEncoder(inputCol=\"NameIndex\", outputCol=\"NameVec\")\n",
    "df_encoded = encoder.fit(df_indexed).transform(df_indexed)\n",
    "\n",
    "# Define the stages of the pipeline\n",
    "stages = [indexer, encoder, LogisticRegression(featuresCol=\"NameVec\", labelCol=\"Label\")]\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline(stages=stages)\n",
    "\n",
    "# Fit the pipeline model\n",
    "model = pipeline.fit(df_encoded)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(df_encoded)\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"Label\")\n",
    "\n",
    "# Evaluate model\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
